{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Antho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "rock = pd.read_csv('./data/rock_round2.csv')\n",
    "rap = pd.read_csv('./data/rap_round2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(993, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get shape of rock dataframe\n",
    "rock.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(996, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get shape of rap dataframe\n",
    "rap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>age</th>\n",
       "      <th>thread</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Funkadelic - Maggot Brain</td>\n",
       "      <td>16</td>\n",
       "      <td>841.220417</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Motorhead - We Are Motorhead (Live Germany 2004)</td>\n",
       "      <td>0</td>\n",
       "      <td>289.603750</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Which bands do you prefer?</td>\n",
       "      <td>5</td>\n",
       "      <td>125.553750</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Anyone with a special affinity towards lengthi...</td>\n",
       "      <td>12</td>\n",
       "      <td>654.987084</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Jebediah - “Jerks Of Attention” (1997)</td>\n",
       "      <td>0</td>\n",
       "      <td>181.837084</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  comments         age  \\\n",
       "0                          Funkadelic - Maggot Brain        16  841.220417   \n",
       "1   Motorhead - We Are Motorhead (Live Germany 2004)         0  289.603750   \n",
       "2                         Which bands do you prefer?         5  125.553750   \n",
       "3  Anyone with a special affinity towards lengthi...        12  654.987084   \n",
       "4             Jebediah - “Jerks Of Attention” (1997)         0  181.837084   \n",
       "\n",
       "  thread  \n",
       "0   rock  \n",
       "1   rock  \n",
       "2   rock  \n",
       "3   rock  \n",
       "4   rock  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first few rows of rock\n",
    "rock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>age</th>\n",
       "      <th>thread</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Kodak v. Corona</td>\n",
       "      <td>13</td>\n",
       "      <td>632.171650</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Had to put it out there</td>\n",
       "      <td>25</td>\n",
       "      <td>1742.321650</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Is this the fastest someone can rap while bein...</td>\n",
       "      <td>0</td>\n",
       "      <td>85.088317</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Amazon Echo: Tyga Edition</td>\n",
       "      <td>0</td>\n",
       "      <td>236.021650</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I would like to see who would win 20 hits for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>92.321650</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  comments          age  \\\n",
       "0                                    Kodak v. Corona        13   632.171650   \n",
       "1                            Had to put it out there        25  1742.321650   \n",
       "2  Is this the fastest someone can rap while bein...         0    85.088317   \n",
       "3                          Amazon Echo: Tyga Edition         0   236.021650   \n",
       "4  I would like to see who would win 20 hits for ...         1    92.321650   \n",
       "\n",
       "  thread  \n",
       "0    rap  \n",
       "1    rap  \n",
       "2    rap  \n",
       "3    rap  \n",
       "4    rap  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first few rows of rap\n",
    "rap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function that will clean a dataframe column\n",
    "def clean_text_column(df_column):\n",
    "    \n",
    "    # Define a list that will contain words in a column of subreddit titles\n",
    "    words_list = [] \n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    df_column = pd.Series([re.sub(\"[^a-zA-Z]\", \" \", BeautifulSoup(text).get_text().lower()) for text in df_column])\n",
    "    \n",
    "    # Instantiate tokenizer\n",
    "    tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "    \n",
    "    # Tokenize each line of the series and append each line (less stopwords) to words_list\n",
    "    for line in df_column:\n",
    "        test = tokenizer.tokenize(line)\n",
    "        words_list.append(' '.join([word for word in test if word not in stopwords.words('english')]))\n",
    "    \n",
    "    # Return the cleaned column as a Pandas series\n",
    "    return pd.Series(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antho\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:357: UserWarning: \"https://soundcloud.com/makaris-delver-274960102/warn-the-masses\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# Clean the two columns in question\n",
    "rock_unique_words = clean_text_column(rock['title'])\n",
    "rap_unique_words = clean_text_column(rap['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                funkadelic maggot brain\n",
       "1                       motorhead motorhead live germany\n",
       "2                                           bands prefer\n",
       "3        anyone special affinity towards lengthier songs\n",
       "4                               jebediah jerks attention\n",
       "                             ...                        \n",
       "988    brother band released first ep think pretty gr...\n",
       "989    little self promotion three piece nj garage ro...\n",
       "990                          hey guys song made acoustic\n",
       "991    first ep needs listeners huge effort need help...\n",
       "992    watch burning windows tramp official music vid...\n",
       "Length: 993, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display unique words in rock\n",
    "rock_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      kodak v corona\n",
       "1                                                 put\n",
       "2      fastest someone rap able understand every word\n",
       "3                            amazon echo tyga edition\n",
       "4         would like see would win hits hits yall got\n",
       "                            ...                      \n",
       "991                    looking vinyl gift please help\n",
       "992                                 found guy youtube\n",
       "993                       dame dash reclaiming throne\n",
       "994                     made new song maybe want hear\n",
       "995                     new playlist original concept\n",
       "Length: 996, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display unique words in rap\n",
    "rap_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that words occuring most frequently in each subreddit do not also overlap between the two subreddits, this data should be adequate for training a model. Roughly 1000 posts for each subreddit will be used as training data and upon first glance at the unique words these data seem to be unique enough for, at the very least, a human to differentiate between the two subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values - Rock Words: 0\n",
      "\n",
      "Null Values - Rap Words: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "print(f'Null Values - Rock Words: {rock_unique_words.isnull().sum()}')\n",
    "print()\n",
    "print(f'Null Values - Rap Words: {rap_unique_words.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type - Rock: object\n",
      "\n",
      "Data Type - Rap: object\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "print(f'Data Type - Rock: {rock_unique_words.dtypes}')\n",
    "print()\n",
    "print(f'Data Type - Rap: {rap_unique_words.dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the original titles with titles containing only unique words\n",
    "rock['title'] = rock_unique_words\n",
    "rap['title'] = rap_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null values unexpectedly appeared during the modeling phase; double check their presence here.\n",
    "rock['title'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null values unexpectedly appeared during the modeling phase; double check their presence here.\n",
    "rap['title'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was only ran once; the files 'rock_clean.csv' and 'rap_clean.csv' contain the original cleaned data used for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .csv files for two subreddits\n",
    "rock.to_csv('./data/rock_clean_round2.csv', index=False)\n",
    "rap.to_csv('./data/rap_clean_round2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
